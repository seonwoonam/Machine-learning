{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST(필기체 숫자) 인식 - 오차역전파 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 60,000개의 training data와 10,000개의 test data를 2차원 행렬 데이터 타입으로 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data.shpae = (60000, 785) , test_data.shape =  (10000, 785)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "training_data = np.loadtxt('./mnist_train.csv',delimiter=',',dtype=np.float32)\n",
    "test_data = np.loadtxt('./mnist_test.csv',delimiter=',',dtype=np.float32)\n",
    "\n",
    "print(\"training_data.shpae =\", training_data.shape, \", test_data.shape = \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 첫번째 열에는 정답이 있고, 나머지 784개의 수는 이미지의 정보\n",
    "- 아래 코드는 이미지 나타내보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img = training_data[0][1:].reshape(28,28)\n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## external function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시그모이드 함수\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuralNetwork class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime # 학습경과시간 측정\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):  # 가중치/ 바이어스/ 각 층 출력값/ 학습율 초기화\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        #은닉층 가중치 W2 = (784 x 100) 의 shape으로 초기화 xavier/he 방법으로 가중치 초기화 \n",
    "        self.W2 = np.random.randn(self.input_nodes, self.hidden_nodes)/np.sqrt(self.input_nodes/2)\n",
    "        self.b2 = np.random.rand(self.hidden_nodes)\n",
    "        \n",
    "        #출력층 가중치 W3 = (100 X 10)\n",
    "        self.W3 = np.random.randn(self.hidden_nodes, self.output_nodes)/np.sqrt(self.hidden_nodes/2)\n",
    "        self.b3 = np.random.rand(self.output_nodes)\n",
    "        \n",
    "        #선형회귀값(sigmoid에 들어가기전 값), 출력값 초기화\n",
    "        self.Z3 = np.zeros([1,output_nodes])\n",
    "        self.A3 = np.zeros([1,output_nodes])\n",
    "        \n",
    "        self.Z2 = np.zeros([1,hidden_nodes])\n",
    "        self.A2 = np.zeros([1,hidden_nodes])\n",
    "       \n",
    "        self.Z1 = np.zeros([1,input_nodes])\n",
    "        self.A1 = np.zeros([1,input_nodes])\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def feed_forward(self): #feed forward 이용하여 손실함수 값 계산\n",
    "        delta = 1e-7\n",
    "        \n",
    "        # 입력층 선형회귀 값, 출력값 계산\n",
    "        self.Z1 = self.input_data\n",
    "        self.A1 = self.input_data\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 계산\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 계산\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "        \n",
    "        #cross-entropy\n",
    "        return -np.sum(self.target_data*np.log(self.A3 + delta) + (1-self.target_data)*np.log((1-self.A3)+delta))\n",
    "    \n",
    "        \n",
    "    def loss_val(self): # 손실함수 값 계산 (외부 출력용)\n",
    "        delta = 1e-7\n",
    "        \n",
    "        # 입력층 선형회귀 값, 출력값 계산\n",
    "        self.Z1 = self.input_data\n",
    "        self.A1 = self.input_data\n",
    "        \n",
    "        # 은닉층 선형회귀 값 Z2, 출력값 A2 계산\n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        \n",
    "        # 출력층 선형회귀 값 Z3, 출력값 A3 계산\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "        \n",
    "        #cross-entropy\n",
    "        return -np.sum(self.target_data*np.log(self.A3 + delta) + (1-self.target_data)*np.log((1-self.A3)+delta))\n",
    "    \n",
    "    def train(self,input_data,target_data): # 오차역전파 공식을 이용하여 가중치/바이어스 업데이트\n",
    "        self.target_data = target_data\n",
    "        self.input_data = input_data\n",
    "        \n",
    "        #먼저 feed_forward를 통해서 최종 출력값과 이를 바탕으로 현재의 에러 값 계산\n",
    "        loss_val = self.feed_forward()\n",
    "        \n",
    "        #출력층 loss인 loss_3 구하기\n",
    "        loss_3 = (self.A3 - self.target_data)*self.A3*(1-self.A3)\n",
    "        \n",
    "        #출력층 가중치 W3, 출력층 바이어스 b3 업데이트\n",
    "        self.W3 = self.W3 - self.learning_rate*np.dot(self.A2.T, loss_3)\n",
    "        self.b3 = self.b3 - self.learning_rate*loss_3\n",
    "        \n",
    "        #은닉층 loss인 loss_2 구하기\n",
    "        loss_2 = np.dot(loss_3,self.W3.T)*self.A2*(1-self.A2)\n",
    "        \n",
    "        #은닉층 가중치 W2, 은닉층 바이어스 b2 업데이트\n",
    "        self.W2 = self.W2 - self.learning_rate*np.dot(self.A1.T, loss_2)\n",
    "        self.b2 = self.b2 - self.learning_rate*loss_2\n",
    "        \n",
    "    def predict(self,input_data): #입력 데이터에 대해 미래 값 예측\n",
    "        Z2 = np.dot(input_data,self.W2) + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        \n",
    "        Z3 = np.dot(A2, self.W3) + self.b3\n",
    "        A3 = sigmoid(Z3)\n",
    "        \n",
    "        # 가장 큰 값의 인덱스 반환\n",
    "        predicted_num = np.argmax(A3)\n",
    "        \n",
    "        return predicted_num\n",
    "\n",
    "    def accuracy(self, test_data): #신경망 기반의 딥러닝 아키텍쳐 정확도 측정\n",
    "        matched_list =[]\n",
    "        not_matched_list = []\n",
    "        \n",
    "        for index in range(len(test_data)):\n",
    "            # 정답분리\n",
    "            label = int(test_data[index,0])\n",
    "            \n",
    "            #normalize\n",
    "            # 가끔 입력데이터 초과로 overflow가 발생하므로 최댓값으로 나눠주고 nomarlize 함\n",
    "            data = (test_data[index,1:]/255.0*0.99)+0.01\n",
    "            \n",
    "            predicted_num = self.predict(np.array(data,ndmin=2))\n",
    "            \n",
    "            if label == predicted_num:\n",
    "                matched_list.append(index)\n",
    "            else:\n",
    "                not_matched_list.append(index)\n",
    "        print(\"Current Accuracy = \", 100*(len(matched_list)/len(test_data)),\" %\")\n",
    "        \n",
    "        return matched_list, not_matched_list\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  0 , loss_val =  4.3369184141090855\n",
      "step =  400 , loss_val =  1.4603062144530494\n",
      "step =  800 , loss_val =  1.560838271891869\n",
      "step =  1200 , loss_val =  0.714106282789858\n",
      "step =  1600 , loss_val =  0.9036939347762223\n",
      "step =  2000 , loss_val =  1.6484484656257203\n",
      "step =  2400 , loss_val =  0.6927810735670289\n",
      "step =  2800 , loss_val =  0.8785227461518478\n",
      "step =  3200 , loss_val =  0.8147267925596378\n",
      "step =  3600 , loss_val =  0.6920093692084823\n",
      "step =  4000 , loss_val =  0.9311602711659213\n",
      "step =  4400 , loss_val =  0.8184476581601098\n",
      "step =  4800 , loss_val =  1.057122235006054\n",
      "step =  5200 , loss_val =  0.7595369897952713\n",
      "step =  5600 , loss_val =  1.4183994006333653\n",
      "step =  6000 , loss_val =  0.8006572571069521\n",
      "step =  6400 , loss_val =  0.8981539152378888\n",
      "step =  6800 , loss_val =  0.8784074767665684\n",
      "step =  7200 , loss_val =  0.81595370687027\n",
      "step =  7600 , loss_val =  0.9238256512639631\n",
      "step =  8000 , loss_val =  0.973562632184879\n",
      "step =  8400 , loss_val =  0.7988274537351464\n",
      "step =  8800 , loss_val =  0.8721698756865518\n",
      "step =  9200 , loss_val =  0.8772384774720318\n",
      "step =  9600 , loss_val =  0.89860269353074\n",
      "step =  10000 , loss_val =  0.963185175386851\n",
      "step =  10400 , loss_val =  0.8651251655206469\n",
      "step =  10800 , loss_val =  2.63277656954418\n",
      "step =  11200 , loss_val =  0.9274150389022003\n",
      "step =  11600 , loss_val =  13.124986838706423\n",
      "step =  12000 , loss_val =  0.896014306543008\n",
      "step =  12400 , loss_val =  0.9058842434293307\n",
      "step =  12800 , loss_val =  0.9324541026252544\n",
      "step =  13200 , loss_val =  0.9366322704794526\n",
      "step =  13600 , loss_val =  0.9530405710341582\n",
      "step =  14000 , loss_val =  0.9956100850184083\n",
      "step =  14400 , loss_val =  0.9642209509505391\n",
      "step =  14800 , loss_val =  0.9669744742790273\n",
      "step =  15200 , loss_val =  1.0663489170076315\n",
      "step =  15600 , loss_val =  0.9866388190301625\n",
      "step =  16000 , loss_val =  1.0034555898997148\n",
      "step =  16400 , loss_val =  0.9260456049275596\n",
      "step =  16800 , loss_val =  1.0972303583200975\n",
      "step =  17200 , loss_val =  1.0510394840802042\n",
      "step =  17600 , loss_val =  1.0557874125206743\n",
      "step =  18000 , loss_val =  1.005620450051815\n",
      "step =  18400 , loss_val =  0.8705304677864705\n",
      "step =  18800 , loss_val =  0.9810552891024181\n",
      "step =  19200 , loss_val =  1.0041692452938709\n",
      "step =  19600 , loss_val =  0.9217499716874747\n",
      "step =  20000 , loss_val =  1.0217303111402325\n",
      "step =  20400 , loss_val =  0.9782653293959093\n",
      "step =  20800 , loss_val =  1.0236706993840607\n",
      "step =  21200 , loss_val =  0.8727404442867397\n",
      "step =  21600 , loss_val =  1.010006082682378\n",
      "step =  22000 , loss_val =  1.0095460175312672\n",
      "step =  22400 , loss_val =  0.9500691448636489\n",
      "step =  22800 , loss_val =  1.035776959523129\n",
      "step =  23200 , loss_val =  0.9882785886579105\n",
      "step =  23600 , loss_val =  1.0710073661610706\n",
      "step =  24000 , loss_val =  1.0523253150187986\n",
      "step =  24400 , loss_val =  1.0930030435119718\n",
      "step =  24800 , loss_val =  0.9384422971350388\n",
      "step =  25200 , loss_val =  1.0142169254487232\n",
      "step =  25600 , loss_val =  0.9378426044755301\n",
      "step =  26000 , loss_val =  0.9578531509325429\n",
      "step =  26400 , loss_val =  1.0037005907059813\n",
      "step =  26800 , loss_val =  1.1604826816838425\n",
      "step =  27200 , loss_val =  1.052841186520941\n",
      "step =  27600 , loss_val =  1.0682284122414651\n",
      "step =  28000 , loss_val =  0.9473750597864427\n",
      "step =  28400 , loss_val =  1.1207451282861325\n",
      "step =  28800 , loss_val =  0.9494867757608066\n",
      "step =  29200 , loss_val =  1.0243922317161418\n",
      "step =  29600 , loss_val =  1.0060227608394432\n",
      "step =  30000 , loss_val =  1.1499630034210613\n",
      "step =  30400 , loss_val =  0.9985947492193847\n",
      "step =  30800 , loss_val =  1.1073554626947508\n",
      "step =  31200 , loss_val =  13.861282115487908\n",
      "step =  31600 , loss_val =  1.0200818892837216\n",
      "step =  32000 , loss_val =  1.0326296052679929\n",
      "step =  32400 , loss_val =  1.046403281295495\n",
      "step =  32800 , loss_val =  1.1251730514547482\n",
      "step =  33200 , loss_val =  1.0367625832718566\n",
      "step =  33600 , loss_val =  1.0139988312515253\n",
      "step =  34000 , loss_val =  0.9725499646906428\n",
      "step =  34400 , loss_val =  1.1232771726634758\n",
      "step =  34800 , loss_val =  9.726078552837016\n",
      "step =  35200 , loss_val =  1.1670473309570928\n",
      "step =  35600 , loss_val =  1.0245736624294446\n",
      "step =  36000 , loss_val =  0.9206825450965577\n",
      "step =  36400 , loss_val =  1.075553119381407\n",
      "step =  36800 , loss_val =  1.0749932736701926\n",
      "step =  37200 , loss_val =  0.9859192041043799\n",
      "step =  37600 , loss_val =  1.031589355963924\n",
      "step =  38000 , loss_val =  1.0197942992324676\n",
      "step =  38400 , loss_val =  1.1424894298895938\n",
      "step =  38800 , loss_val =  1.0629063941445298\n",
      "step =  39200 , loss_val =  1.0922641765221737\n",
      "step =  39600 , loss_val =  1.3902034212170504\n",
      "step =  40000 , loss_val =  1.069687370566797\n",
      "step =  40400 , loss_val =  1.156327119205841\n",
      "step =  40800 , loss_val =  0.9946557417254727\n",
      "step =  41200 , loss_val =  1.0181101062840052\n",
      "step =  41600 , loss_val =  0.9750071086877969\n",
      "step =  42000 , loss_val =  1.0017306779720174\n",
      "step =  42400 , loss_val =  0.998038267524544\n",
      "step =  42800 , loss_val =  1.0553831330534538\n",
      "step =  43200 , loss_val =  1.1441147450933058\n",
      "step =  43600 , loss_val =  0.9401247925522414\n",
      "step =  44000 , loss_val =  1.1357211026988694\n",
      "step =  44400 , loss_val =  1.0985509921889887\n",
      "step =  44800 , loss_val =  0.9843639203569846\n",
      "step =  45200 , loss_val =  1.1946287705383003\n",
      "step =  45600 , loss_val =  1.050528629152915\n",
      "step =  46000 , loss_val =  1.0436455773539075\n",
      "step =  46400 , loss_val =  1.0538116651299678\n",
      "step =  46800 , loss_val =  1.1365397805649229\n",
      "step =  47200 , loss_val =  1.0357774673702593\n",
      "step =  47600 , loss_val =  2.99521521512012\n",
      "step =  48000 , loss_val =  1.1469503963360563\n",
      "step =  48400 , loss_val =  1.1408279425832057\n",
      "step =  48800 , loss_val =  1.0597687748052744\n",
      "step =  49200 , loss_val =  1.3186800300473966\n",
      "step =  49600 , loss_val =  1.003409422310895\n",
      "step =  50000 , loss_val =  1.0854903736280208\n",
      "step =  50400 , loss_val =  1.0462532681510777\n",
      "step =  50800 , loss_val =  1.1975701375171701\n",
      "step =  51200 , loss_val =  0.9419253852659721\n",
      "step =  51600 , loss_val =  7.615869475923556\n",
      "step =  52000 , loss_val =  1.0936794083413366\n",
      "step =  52400 , loss_val =  1.14923807054874\n",
      "step =  52800 , loss_val =  7.774770801347389\n",
      "step =  53200 , loss_val =  1.1154732003149366\n",
      "step =  53600 , loss_val =  1.076258911311367\n",
      "step =  54000 , loss_val =  1.0791619467290992\n",
      "step =  54400 , loss_val =  1.0696283395831707\n",
      "step =  54800 , loss_val =  1.1819441442577368\n",
      "step =  55200 , loss_val =  1.0711438761305412\n",
      "step =  55600 , loss_val =  1.0682395620458986\n",
      "step =  56000 , loss_val =  1.0364920444155183\n",
      "step =  56400 , loss_val =  1.1193627669041075\n",
      "step =  56800 , loss_val =  14.989967959984675\n",
      "step =  57200 , loss_val =  1.0909088370461382\n",
      "step =  57600 , loss_val =  1.1263605106980181\n",
      "step =  58000 , loss_val =  1.1513788207364577\n",
      "step =  58400 , loss_val =  1.0211561600745391\n",
      "step =  58800 , loss_val =  1.1828076911014909\n",
      "step =  59200 , loss_val =  1.00654608572996\n",
      "step =  59600 , loss_val =  1.2079479108891982\n",
      "step =  0 , loss_val =  1.2344523597040318\n",
      "step =  400 , loss_val =  1.24599349356185\n",
      "step =  800 , loss_val =  1.1177137437209768\n",
      "step =  1200 , loss_val =  1.049295075918927\n",
      "step =  1600 , loss_val =  1.0629377237025772\n",
      "step =  2000 , loss_val =  1.009606777909783\n",
      "step =  2400 , loss_val =  1.1361713390661756\n",
      "step =  2800 , loss_val =  1.014826204454353\n",
      "step =  3200 , loss_val =  1.1279024589182463\n",
      "step =  3600 , loss_val =  1.0178074691498193\n",
      "step =  4000 , loss_val =  1.1975204707448361\n",
      "step =  4400 , loss_val =  1.0800866663267612\n",
      "step =  4800 , loss_val =  0.9613843797094751\n",
      "step =  5200 , loss_val =  1.0979726660546143\n",
      "step =  5600 , loss_val =  1.1152010611238095\n",
      "step =  6000 , loss_val =  1.1885841006171824\n",
      "step =  6400 , loss_val =  1.1858884312146798\n",
      "step =  6800 , loss_val =  1.0586119188434748\n",
      "step =  7200 , loss_val =  1.0682927384941128\n",
      "step =  7600 , loss_val =  1.1163539719040507\n",
      "step =  8000 , loss_val =  1.270894098795927\n",
      "step =  8400 , loss_val =  0.9299594761539481\n",
      "step =  8800 , loss_val =  1.280020321174617\n",
      "step =  9200 , loss_val =  1.1087657556123183\n",
      "step =  9600 , loss_val =  1.2827603606872446\n",
      "step =  10000 , loss_val =  1.2613984773022306\n",
      "step =  10400 , loss_val =  1.1053717531555325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  10800 , loss_val =  1.1797364743271745\n",
      "step =  11200 , loss_val =  1.0247690939008698\n",
      "step =  11600 , loss_val =  3.493421994657481\n",
      "step =  12000 , loss_val =  1.1678253652963282\n",
      "step =  12400 , loss_val =  0.9851210687932064\n",
      "step =  12800 , loss_val =  1.1450805941189315\n",
      "step =  13200 , loss_val =  1.0436508317922606\n",
      "step =  13600 , loss_val =  1.26076933841702\n",
      "step =  14000 , loss_val =  1.0807443753970023\n",
      "step =  14400 , loss_val =  1.1125544737701103\n",
      "step =  14800 , loss_val =  1.1271230563515815\n",
      "step =  15200 , loss_val =  1.2797832012515682\n",
      "step =  15600 , loss_val =  1.1479869009318198\n",
      "step =  16000 , loss_val =  1.2320695004101627\n",
      "step =  16400 , loss_val =  1.166041646113447\n",
      "step =  16800 , loss_val =  1.2551657199198936\n",
      "step =  17200 , loss_val =  1.1707433740556406\n",
      "step =  17600 , loss_val =  1.1089832496406689\n",
      "step =  18000 , loss_val =  1.1539156461204865\n",
      "step =  18400 , loss_val =  1.042241125704806\n",
      "step =  18800 , loss_val =  1.0230330425018532\n",
      "step =  19200 , loss_val =  1.2243814562900506\n",
      "step =  19600 , loss_val =  1.1310697163114847\n",
      "step =  20000 , loss_val =  1.1926759995735532\n",
      "step =  20400 , loss_val =  1.1502221721152273\n",
      "step =  20800 , loss_val =  1.2264022528932763\n",
      "step =  21200 , loss_val =  1.004889762278409\n",
      "step =  21600 , loss_val =  1.1752235938866151\n",
      "step =  22000 , loss_val =  1.1923381575065717\n",
      "step =  22400 , loss_val =  1.1034825461013191\n",
      "step =  22800 , loss_val =  1.1233005829300515\n",
      "step =  23200 , loss_val =  1.0643667283122844\n",
      "step =  23600 , loss_val =  1.0755848289229613\n",
      "step =  24000 , loss_val =  1.1470119688881135\n",
      "step =  24400 , loss_val =  1.2993166026929883\n",
      "step =  24800 , loss_val =  1.0289448057734358\n",
      "step =  25200 , loss_val =  1.065330862513196\n",
      "step =  25600 , loss_val =  1.0710569229583127\n",
      "step =  26000 , loss_val =  1.160512466487703\n",
      "step =  26400 , loss_val =  1.2203266021800476\n",
      "step =  26800 , loss_val =  1.338359044665323\n",
      "step =  27200 , loss_val =  1.0563179613979963\n",
      "step =  27600 , loss_val =  1.1762676560622145\n",
      "step =  28000 , loss_val =  1.062158375500949\n",
      "step =  28400 , loss_val =  1.1789982320924004\n",
      "step =  28800 , loss_val =  1.1280072741197922\n",
      "step =  29200 , loss_val =  1.0893305056010347\n",
      "step =  29600 , loss_val =  1.1227499382686241\n",
      "step =  30000 , loss_val =  1.2628412629193506\n",
      "step =  30400 , loss_val =  1.0261693845055273\n",
      "step =  30800 , loss_val =  1.1499637106802063\n",
      "step =  31200 , loss_val =  12.54636433802072\n",
      "step =  31600 , loss_val =  3.659668839704496\n",
      "step =  32000 , loss_val =  1.1341515068314694\n",
      "step =  32400 , loss_val =  1.1612137938209919\n",
      "step =  32800 , loss_val =  1.1955004980551838\n",
      "step =  33200 , loss_val =  1.1214016260123953\n",
      "step =  33600 , loss_val =  1.0138531342693242\n",
      "step =  34000 , loss_val =  1.074302339728015\n",
      "step =  34400 , loss_val =  1.189039638810125\n",
      "step =  34800 , loss_val =  6.2648633987056686\n",
      "step =  35200 , loss_val =  1.1881568567727545\n",
      "step =  35600 , loss_val =  1.0131871017107754\n",
      "step =  36000 , loss_val =  1.0364516605436727\n",
      "step =  36400 , loss_val =  1.1860314966185834\n",
      "step =  36800 , loss_val =  1.1553149310574637\n",
      "step =  37200 , loss_val =  1.1464663254535667\n",
      "step =  37600 , loss_val =  1.0718870604989772\n",
      "step =  38000 , loss_val =  1.0874714090173885\n",
      "step =  38400 , loss_val =  1.185434151835967\n",
      "step =  38800 , loss_val =  1.2620849636166218\n",
      "step =  39200 , loss_val =  1.23747754466494\n",
      "step =  39600 , loss_val =  1.0611230675971492\n",
      "step =  40000 , loss_val =  1.1964522983418404\n",
      "step =  40400 , loss_val =  1.2957334944869974\n",
      "step =  40800 , loss_val =  1.1012319643815642\n",
      "step =  41200 , loss_val =  1.0886905613483462\n",
      "step =  41600 , loss_val =  1.045760768720784\n",
      "step =  42000 , loss_val =  1.1424021702790579\n",
      "step =  42400 , loss_val =  1.1050315142360927\n",
      "step =  42800 , loss_val =  1.1842872654198635\n",
      "step =  43200 , loss_val =  1.1777173693806058\n",
      "step =  43600 , loss_val =  1.1372653291091996\n",
      "step =  44000 , loss_val =  1.2542484326743175\n",
      "step =  44400 , loss_val =  1.163390709426884\n",
      "step =  44800 , loss_val =  1.0686330758141804\n",
      "step =  45200 , loss_val =  1.3373807886393378\n",
      "step =  45600 , loss_val =  1.1852657791722034\n",
      "step =  46000 , loss_val =  1.190337051854563\n",
      "step =  46400 , loss_val =  1.0968154945041568\n",
      "step =  46800 , loss_val =  1.1009759163055928\n",
      "step =  47200 , loss_val =  1.1787225709963056\n",
      "step =  47600 , loss_val =  3.9734713302142946\n",
      "step =  48000 , loss_val =  1.2106067012727346\n",
      "step =  48400 , loss_val =  1.224537557584527\n",
      "step =  48800 , loss_val =  1.1262252503094208\n",
      "step =  49200 , loss_val =  1.1790855604415231\n",
      "step =  49600 , loss_val =  1.050431080498375\n",
      "step =  50000 , loss_val =  1.1591273661983785\n",
      "step =  50400 , loss_val =  1.0399060450298718\n",
      "step =  50800 , loss_val =  1.2820783672888059\n",
      "step =  51200 , loss_val =  0.9969627588039499\n",
      "step =  51600 , loss_val =  7.237788626367683\n",
      "step =  52000 , loss_val =  1.218203877545496\n",
      "step =  52400 , loss_val =  1.1991274437210786\n",
      "step =  52800 , loss_val =  10.398523082372803\n",
      "step =  53200 , loss_val =  1.1297022007285522\n",
      "step =  53600 , loss_val =  1.0904137856060687\n",
      "step =  54000 , loss_val =  1.2250265912309073\n",
      "step =  54400 , loss_val =  1.1687901126016746\n",
      "step =  54800 , loss_val =  1.210744311824599\n",
      "step =  55200 , loss_val =  1.1388997156891782\n",
      "step =  55600 , loss_val =  1.1263538959860524\n",
      "step =  56000 , loss_val =  1.1270022302941844\n",
      "step =  56400 , loss_val =  1.238024188012975\n",
      "step =  56800 , loss_val =  19.353565152499375\n",
      "step =  57200 , loss_val =  1.1625436877651194\n",
      "step =  57600 , loss_val =  1.1722701776527995\n",
      "step =  58000 , loss_val =  1.210491309788097\n",
      "step =  58400 , loss_val =  1.0749533703507228\n",
      "step =  58800 , loss_val =  1.1887496986176647\n",
      "step =  59200 , loss_val =  1.0938722450939524\n",
      "step =  59600 , loss_val =  1.2733565016697201\n",
      "step =  0 , loss_val =  1.2081376106483617\n",
      "step =  400 , loss_val =  1.2900665170780283\n",
      "step =  800 , loss_val =  1.1106918138807922\n",
      "step =  1200 , loss_val =  1.161119584602337\n",
      "step =  1600 , loss_val =  1.0563364458491873\n",
      "step =  2000 , loss_val =  1.0913436206628806\n",
      "step =  2400 , loss_val =  1.2160848610782051\n",
      "step =  2800 , loss_val =  1.1289829749351779\n",
      "step =  3200 , loss_val =  1.1588492189280404\n",
      "step =  3600 , loss_val =  1.1205448703677001\n",
      "step =  4000 , loss_val =  1.208309896134426\n",
      "step =  4400 , loss_val =  1.1470697817645643\n",
      "step =  4800 , loss_val =  1.1030159469694512\n",
      "step =  5200 , loss_val =  1.1431359383371986\n",
      "step =  5600 , loss_val =  1.6022544898187585\n",
      "step =  6000 , loss_val =  1.2287312500600074\n",
      "step =  6400 , loss_val =  1.280705238947912\n",
      "step =  6800 , loss_val =  1.1397894101446955\n",
      "step =  7200 , loss_val =  1.0095313516102173\n",
      "step =  7600 , loss_val =  1.1618982663092061\n",
      "step =  8000 , loss_val =  1.340666242209229\n",
      "step =  8400 , loss_val =  1.0371320927972536\n",
      "step =  8800 , loss_val =  1.2350425884286047\n",
      "step =  9200 , loss_val =  1.2904275845656303\n",
      "step =  9600 , loss_val =  1.3846424128592145\n",
      "step =  10000 , loss_val =  1.3133830599688379\n",
      "step =  10400 , loss_val =  1.1570960084293018\n",
      "step =  10800 , loss_val =  1.162983132247336\n",
      "step =  11200 , loss_val =  1.1434388051608813\n",
      "step =  11600 , loss_val =  1.2296470453508577\n",
      "step =  12000 , loss_val =  1.1470273443105514\n",
      "step =  12400 , loss_val =  1.0032193144050365\n",
      "step =  12800 , loss_val =  1.16159445358496\n",
      "step =  13200 , loss_val =  1.1029020774960219\n",
      "step =  13600 , loss_val =  1.3031992354513688\n",
      "step =  14000 , loss_val =  1.0619637329295168\n",
      "step =  14400 , loss_val =  1.185480361641405\n",
      "step =  14800 , loss_val =  1.1270794899931598\n",
      "step =  15200 , loss_val =  1.3512938812794895\n",
      "step =  15600 , loss_val =  1.2112234165233908\n",
      "step =  16000 , loss_val =  1.1255543806633088\n",
      "step =  16400 , loss_val =  1.2224773939240934\n",
      "step =  16800 , loss_val =  1.2581503619545211\n",
      "step =  17200 , loss_val =  1.1515450831932508\n",
      "step =  17600 , loss_val =  1.1514303754292492\n",
      "step =  18000 , loss_val =  1.1476461996172047\n",
      "step =  18400 , loss_val =  1.0902216886638312\n",
      "step =  18800 , loss_val =  1.2061730668892865\n",
      "step =  19200 , loss_val =  1.2429365559662613\n",
      "step =  19600 , loss_val =  1.1723355730153742\n",
      "step =  20000 , loss_val =  1.286054634368853\n",
      "step =  20400 , loss_val =  1.2333534269873052\n",
      "step =  20800 , loss_val =  1.2406416720547682\n",
      "step =  21200 , loss_val =  1.0894063823035218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  21600 , loss_val =  1.2729892274931005\n",
      "step =  22000 , loss_val =  1.203944085186898\n",
      "step =  22400 , loss_val =  1.1574856478369917\n",
      "step =  22800 , loss_val =  1.1922572838231469\n",
      "step =  23200 , loss_val =  1.1113336129442168\n",
      "step =  23600 , loss_val =  1.1730052111701734\n",
      "step =  24000 , loss_val =  1.1750802651294867\n",
      "step =  24400 , loss_val =  1.2911786562800553\n",
      "step =  24800 , loss_val =  1.0228424129950568\n",
      "step =  25200 , loss_val =  1.0987915055393538\n",
      "step =  25600 , loss_val =  1.170982083096395\n",
      "step =  26000 , loss_val =  1.165675925413755\n",
      "step =  26400 , loss_val =  1.1956487657339077\n",
      "step =  26800 , loss_val =  1.3994743803837275\n",
      "step =  27200 , loss_val =  1.087677296461491\n",
      "step =  27600 , loss_val =  1.33623813530305\n",
      "step =  28000 , loss_val =  1.0521017174720089\n",
      "step =  28400 , loss_val =  1.2688360071564313\n",
      "step =  28800 , loss_val =  1.1278084234534662\n",
      "step =  29200 , loss_val =  1.1837300557647343\n",
      "step =  29600 , loss_val =  1.2164810753565016\n",
      "step =  30000 , loss_val =  1.2691280098328983\n",
      "step =  30400 , loss_val =  1.0772018730531419\n",
      "step =  30800 , loss_val =  1.200937411626524\n",
      "step =  31200 , loss_val =  9.111045139440321\n",
      "step =  31600 , loss_val =  1.0813908830356638\n",
      "step =  32000 , loss_val =  1.2213578390079907\n",
      "step =  32400 , loss_val =  1.1384456323533623\n",
      "step =  32800 , loss_val =  1.2202732348365932\n",
      "step =  33200 , loss_val =  1.196845983044407\n",
      "step =  33600 , loss_val =  1.1077409101790079\n",
      "step =  34000 , loss_val =  1.140417586511766\n",
      "step =  34400 , loss_val =  1.2915819054186155\n",
      "step =  34800 , loss_val =  1.88693510350163\n",
      "step =  35200 , loss_val =  1.2164261278534223\n",
      "step =  35600 , loss_val =  1.0199131605197242\n",
      "step =  36000 , loss_val =  1.0130289027117403\n",
      "step =  36400 , loss_val =  1.1923501838698647\n",
      "step =  36800 , loss_val =  1.1652035374161744\n",
      "step =  37200 , loss_val =  1.130697832109169\n",
      "step =  37600 , loss_val =  1.1434753525462036\n",
      "step =  38000 , loss_val =  1.24945149525742\n",
      "step =  38400 , loss_val =  1.3000489465656884\n",
      "step =  38800 , loss_val =  1.2299932508206126\n",
      "step =  39200 , loss_val =  1.3210308215193125\n",
      "step =  39600 , loss_val =  1.7476570016097994\n",
      "step =  40000 , loss_val =  1.2575011705677233\n",
      "step =  40400 , loss_val =  1.3412969288810732\n",
      "step =  40800 , loss_val =  1.0975769189592877\n",
      "step =  41200 , loss_val =  1.1222005114280993\n",
      "step =  41600 , loss_val =  1.0519632188038612\n",
      "step =  42000 , loss_val =  1.1655076785910057\n",
      "step =  42400 , loss_val =  1.1730775208398714\n",
      "step =  42800 , loss_val =  1.1935323312028379\n",
      "step =  43200 , loss_val =  1.2377865059509483\n",
      "step =  43600 , loss_val =  1.1105147604559586\n",
      "step =  44000 , loss_val =  1.2490288348227259\n",
      "step =  44400 , loss_val =  1.25511665301848\n",
      "step =  44800 , loss_val =  1.1382332372459736\n",
      "step =  45200 , loss_val =  1.3337519722555915\n",
      "step =  45600 , loss_val =  1.1860118277025316\n",
      "step =  46000 , loss_val =  1.294320818376393\n",
      "step =  46400 , loss_val =  1.0842597967973255\n",
      "step =  46800 , loss_val =  1.1639600930395944\n",
      "step =  47200 , loss_val =  1.2014322980868408\n",
      "step =  47600 , loss_val =  6.134826686483594\n",
      "step =  48000 , loss_val =  1.1300121404703252\n",
      "step =  48400 , loss_val =  1.3123789902059748\n",
      "step =  48800 , loss_val =  1.1446028990784223\n",
      "step =  49200 , loss_val =  1.2026342685077338\n",
      "step =  49600 , loss_val =  1.098802039034389\n",
      "step =  50000 , loss_val =  1.2185759129950062\n",
      "step =  50400 , loss_val =  1.0521845023179535\n",
      "step =  50800 , loss_val =  1.363789713627205\n",
      "step =  51200 , loss_val =  1.13223278909636\n",
      "step =  51600 , loss_val =  9.614757479631166\n",
      "step =  52000 , loss_val =  1.2777082833614113\n",
      "step =  52400 , loss_val =  1.2685206945239087\n",
      "step =  52800 , loss_val =  15.921608047199603\n",
      "step =  53200 , loss_val =  1.236721176608349\n",
      "step =  53600 , loss_val =  1.1986338138507886\n",
      "step =  54000 , loss_val =  1.2778574499917654\n",
      "step =  54400 , loss_val =  1.2139088258177213\n",
      "step =  54800 , loss_val =  1.254310276588288\n",
      "step =  55200 , loss_val =  1.1219424723361082\n",
      "step =  55600 , loss_val =  1.2012564174482148\n",
      "step =  56000 , loss_val =  1.0654882779812378\n",
      "step =  56400 , loss_val =  1.218518447525758\n",
      "step =  56800 , loss_val =  21.055030614027462\n",
      "step =  57200 , loss_val =  1.2593854557310935\n",
      "step =  57600 , loss_val =  1.1912858978844938\n",
      "step =  58000 , loss_val =  1.272832622905525\n",
      "step =  58400 , loss_val =  1.1388704217831918\n",
      "step =  58800 , loss_val =  1.163290793671423\n",
      "step =  59200 , loss_val =  1.1860071688678568\n",
      "step =  59600 , loss_val =  1.3589805257917504\n",
      "step =  0 , loss_val =  1.2381862326026947\n",
      "step =  400 , loss_val =  1.2962089231369174\n",
      "step =  800 , loss_val =  1.163032793938546\n",
      "step =  1200 , loss_val =  1.220033218579787\n",
      "step =  1600 , loss_val =  1.2002677144865088\n",
      "step =  2000 , loss_val =  1.1601371382061174\n",
      "step =  2400 , loss_val =  1.290565045442553\n",
      "step =  2800 , loss_val =  1.135510885671362\n",
      "step =  3200 , loss_val =  1.2337109587961501\n",
      "step =  3600 , loss_val =  1.177771008176265\n",
      "step =  4000 , loss_val =  1.2178124503036367\n",
      "step =  4400 , loss_val =  1.2474429575517512\n",
      "step =  4800 , loss_val =  1.1403408134911017\n",
      "step =  5200 , loss_val =  1.1903944347780124\n",
      "step =  5600 , loss_val =  1.072128506615517\n",
      "step =  6000 , loss_val =  1.2948822987848432\n",
      "step =  6400 , loss_val =  1.3563646486506629\n",
      "step =  6800 , loss_val =  1.23016764207813\n",
      "step =  7200 , loss_val =  1.0516613751921602\n",
      "step =  7600 , loss_val =  1.3173967810805631\n",
      "step =  8000 , loss_val =  1.3721260148780663\n",
      "step =  8400 , loss_val =  1.0182551143332352\n",
      "step =  8800 , loss_val =  1.2921405415344842\n",
      "step =  9200 , loss_val =  1.2861399234779305\n",
      "step =  9600 , loss_val =  1.4063288040908501\n",
      "step =  10000 , loss_val =  1.3387843888563122\n",
      "step =  10400 , loss_val =  1.198461397994255\n",
      "step =  10800 , loss_val =  1.0242414733655747\n",
      "step =  11200 , loss_val =  1.2399098998488873\n",
      "step =  11600 , loss_val =  1.3926666946280946\n",
      "step =  12000 , loss_val =  1.17063192153732\n",
      "step =  12400 , loss_val =  1.009855188171303\n",
      "step =  12800 , loss_val =  1.2920498102034623\n",
      "step =  13200 , loss_val =  1.1515051585139333\n",
      "step =  13600 , loss_val =  1.3575508486219185\n",
      "step =  14000 , loss_val =  1.1328400440332007\n",
      "step =  14400 , loss_val =  1.2046057504542504\n",
      "step =  14800 , loss_val =  1.1593517956162185\n",
      "step =  15200 , loss_val =  1.3919557549102284\n",
      "step =  15600 , loss_val =  1.2813373418626595\n",
      "step =  16000 , loss_val =  1.2578796191891044\n",
      "step =  16400 , loss_val =  1.2881425969679918\n",
      "step =  16800 , loss_val =  1.348291010365515\n",
      "step =  17200 , loss_val =  1.1958756807787665\n",
      "step =  17600 , loss_val =  1.1460185807796446\n",
      "step =  18000 , loss_val =  1.1640147569348\n",
      "step =  18400 , loss_val =  1.1678221954835817\n",
      "step =  18800 , loss_val =  1.1727592685918327\n",
      "step =  19200 , loss_val =  1.2207088799697363\n",
      "step =  19600 , loss_val =  1.1881574358648213\n",
      "step =  20000 , loss_val =  1.3813545709796817\n",
      "step =  20400 , loss_val =  1.3588969970220708\n",
      "step =  20800 , loss_val =  1.2622565920357731\n",
      "step =  21200 , loss_val =  1.086033275521551\n",
      "step =  21600 , loss_val =  1.280408328355682\n",
      "step =  22000 , loss_val =  1.2161330240083932\n",
      "step =  22400 , loss_val =  1.1684315751481242\n",
      "step =  22800 , loss_val =  1.135957355745639\n",
      "step =  23200 , loss_val =  1.1452702000393462\n",
      "step =  23600 , loss_val =  1.1366507697882957\n",
      "step =  24000 , loss_val =  1.164631848360565\n",
      "step =  24400 , loss_val =  1.2556538403457125\n",
      "step =  24800 , loss_val =  1.060681752831846\n",
      "step =  25200 , loss_val =  1.1387038229786055\n",
      "step =  25600 , loss_val =  1.1615361849542603\n",
      "step =  26000 , loss_val =  1.2786342426734283\n",
      "step =  26400 , loss_val =  1.1677750683333987\n",
      "step =  26800 , loss_val =  1.512206463397086\n",
      "step =  27200 , loss_val =  1.2025473421192188\n",
      "step =  27600 , loss_val =  1.4351575226368078\n",
      "step =  28000 , loss_val =  1.1028707415300794\n",
      "step =  28400 , loss_val =  1.38402527497332\n",
      "step =  28800 , loss_val =  1.2041685155892417\n",
      "step =  29200 , loss_val =  1.271066704219351\n",
      "step =  29600 , loss_val =  1.2283249837676322\n",
      "step =  30000 , loss_val =  1.3150001617291123\n",
      "step =  30400 , loss_val =  1.085242167231771\n",
      "step =  30800 , loss_val =  1.1765050069321705\n",
      "step =  31200 , loss_val =  5.502415674945258\n",
      "step =  31600 , loss_val =  1.1911710785737613\n",
      "step =  32000 , loss_val =  1.3195965371478644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  32400 , loss_val =  1.1812181131528412\n",
      "step =  32800 , loss_val =  1.2504075282251974\n",
      "step =  33200 , loss_val =  1.2063031616257431\n",
      "step =  33600 , loss_val =  1.126500366463678\n",
      "step =  34000 , loss_val =  1.1801715625676883\n",
      "step =  34400 , loss_val =  1.3017100347649544\n",
      "step =  34800 , loss_val =  9.171391787218111\n",
      "step =  35200 , loss_val =  1.2440911125862435\n",
      "step =  35600 , loss_val =  1.093190813450391\n",
      "step =  36000 , loss_val =  1.1120950707178885\n",
      "step =  36400 , loss_val =  1.2449123303489462\n",
      "step =  36800 , loss_val =  1.2201861242809515\n",
      "step =  37200 , loss_val =  1.1508103505055365\n",
      "step =  37600 , loss_val =  1.1967307450973093\n",
      "step =  38000 , loss_val =  1.218758138769329\n",
      "step =  38400 , loss_val =  1.3437792959793295\n",
      "step =  38800 , loss_val =  1.236122621318076\n",
      "step =  39200 , loss_val =  1.322627296810972\n",
      "step =  39600 , loss_val =  1.0961760842683872\n",
      "step =  40000 , loss_val =  1.212392129020158\n",
      "step =  40400 , loss_val =  1.3854145288337116\n",
      "step =  40800 , loss_val =  1.1449818950473882\n",
      "step =  41200 , loss_val =  1.1991287509144102\n",
      "step =  41600 , loss_val =  1.1125494237198428\n",
      "step =  42000 , loss_val =  1.1958753315288944\n",
      "step =  42400 , loss_val =  1.1643957630427606\n",
      "step =  42800 , loss_val =  1.319134718594732\n",
      "step =  43200 , loss_val =  1.2860525989741287\n",
      "step =  43600 , loss_val =  1.1917495651339043\n",
      "step =  44000 , loss_val =  1.3233090155282785\n",
      "step =  44400 , loss_val =  1.2904370652737975\n",
      "step =  44800 , loss_val =  1.1648839889047937\n",
      "step =  45200 , loss_val =  1.4229159707984849\n",
      "step =  45600 , loss_val =  1.3057359999110147\n",
      "step =  46000 , loss_val =  1.1873208196011933\n",
      "step =  46400 , loss_val =  1.1902449750691424\n",
      "step =  46800 , loss_val =  1.166205909224188\n",
      "step =  47200 , loss_val =  1.1867643528320078\n",
      "step =  47600 , loss_val =  13.43773504593309\n",
      "step =  48000 , loss_val =  1.2614728096388768\n",
      "step =  48400 , loss_val =  1.3441303556380435\n",
      "step =  48800 , loss_val =  1.2018069483003102\n",
      "step =  49200 , loss_val =  1.2110675596951028\n",
      "step =  49600 , loss_val =  1.047861419623382\n",
      "step =  50000 , loss_val =  1.235299741285369\n",
      "step =  50400 , loss_val =  1.1347130729768198\n",
      "step =  50800 , loss_val =  1.3605218542020743\n",
      "step =  51200 , loss_val =  1.1464993415182456\n",
      "step =  51600 , loss_val =  8.687071312675975\n",
      "step =  52000 , loss_val =  1.3187103372972158\n",
      "step =  52400 , loss_val =  1.3559750214166402\n",
      "step =  52800 , loss_val =  15.993289104489628\n",
      "step =  53200 , loss_val =  1.2786495921502705\n",
      "step =  53600 , loss_val =  1.2235466002536244\n",
      "step =  54000 , loss_val =  1.2059964540811603\n",
      "step =  54400 , loss_val =  1.2440825169705323\n",
      "step =  54800 , loss_val =  1.3071035761159402\n",
      "step =  55200 , loss_val =  1.124291183519941\n",
      "step =  55600 , loss_val =  1.2444910328235974\n",
      "step =  56000 , loss_val =  1.161324107979678\n",
      "step =  56400 , loss_val =  1.187365306645058\n",
      "step =  56800 , loss_val =  18.562433500059107\n",
      "step =  57200 , loss_val =  1.2714138658657144\n",
      "step =  57600 , loss_val =  1.2779131864439393\n",
      "step =  58000 , loss_val =  1.3248977412988272\n",
      "step =  58400 , loss_val =  1.1638812158864995\n",
      "step =  58800 , loss_val =  1.2390733525624649\n",
      "step =  59200 , loss_val =  1.188720315586391\n",
      "step =  59600 , loss_val =  1.394179235489469\n",
      "step =  0 , loss_val =  1.3055155695448448\n",
      "step =  400 , loss_val =  1.3302283715236272\n",
      "step =  800 , loss_val =  1.2012328479341188\n",
      "step =  1200 , loss_val =  1.135707919154527\n",
      "step =  1600 , loss_val =  1.1609971079105006\n",
      "step =  2000 , loss_val =  1.1086997622973884\n",
      "step =  2400 , loss_val =  1.2998529409629662\n",
      "step =  2800 , loss_val =  1.2095905392533723\n",
      "step =  3200 , loss_val =  1.280042824489187\n",
      "step =  3600 , loss_val =  1.2215465815228717\n",
      "step =  4000 , loss_val =  1.3238666246662336\n",
      "step =  4400 , loss_val =  1.2778747340077357\n",
      "step =  4800 , loss_val =  1.1302266862780583\n",
      "step =  5200 , loss_val =  1.2453640538394408\n",
      "step =  5600 , loss_val =  1.1098537737180163\n",
      "step =  6000 , loss_val =  1.3316054565925328\n",
      "step =  6400 , loss_val =  1.3270284671666435\n",
      "step =  6800 , loss_val =  1.2543015427281068\n",
      "step =  7200 , loss_val =  1.1181656387797583\n",
      "step =  7600 , loss_val =  1.3377193725318597\n",
      "step =  8000 , loss_val =  1.359540433367833\n",
      "step =  8400 , loss_val =  1.086187723603176\n",
      "step =  8800 , loss_val =  1.45244667205924\n",
      "step =  9200 , loss_val =  1.3394820504871296\n",
      "step =  9600 , loss_val =  1.5338301420982514\n",
      "step =  10000 , loss_val =  1.3857389357987047\n",
      "step =  10400 , loss_val =  1.2202299451564464\n",
      "step =  10800 , loss_val =  1.1630426072790714\n",
      "step =  11200 , loss_val =  1.2015157575272262\n",
      "step =  11600 , loss_val =  1.165242640426329\n",
      "step =  12000 , loss_val =  1.1686915738576922\n",
      "step =  12400 , loss_val =  1.1189804309066835\n",
      "step =  12800 , loss_val =  1.3392112935826814\n",
      "step =  13200 , loss_val =  1.2102343090851475\n",
      "step =  13600 , loss_val =  1.3834407425748882\n",
      "step =  14000 , loss_val =  1.195836574033461\n",
      "step =  14400 , loss_val =  1.3049167393056638\n",
      "step =  14800 , loss_val =  1.2434509161667728\n",
      "step =  15200 , loss_val =  1.3702289207979512\n",
      "step =  15600 , loss_val =  1.3567930770285266\n",
      "step =  16000 , loss_val =  1.3344553803978103\n",
      "step =  16400 , loss_val =  1.2978218509181014\n",
      "step =  16800 , loss_val =  1.3435445021082113\n",
      "step =  17200 , loss_val =  1.237468847914507\n",
      "step =  17600 , loss_val =  1.208538673807709\n",
      "step =  18000 , loss_val =  1.20923147164068\n",
      "step =  18400 , loss_val =  1.2065263543151559\n",
      "step =  18800 , loss_val =  1.1203678774312946\n",
      "step =  19200 , loss_val =  1.2528788489510074\n",
      "step =  19600 , loss_val =  1.2021074993584504\n",
      "step =  20000 , loss_val =  1.4265977239884629\n",
      "step =  20400 , loss_val =  1.3099391597443688\n",
      "step =  20800 , loss_val =  1.223717870674911\n",
      "step =  21200 , loss_val =  1.1456793356749897\n",
      "step =  21600 , loss_val =  1.3036924541699961\n",
      "step =  22000 , loss_val =  1.2293484568641002\n",
      "step =  22400 , loss_val =  1.2131681694480356\n",
      "step =  22800 , loss_val =  1.2140407196243261\n",
      "step =  23200 , loss_val =  1.245187156813745\n",
      "step =  23600 , loss_val =  1.1659336988845626\n",
      "step =  24000 , loss_val =  1.1909994577257075\n",
      "step =  24400 , loss_val =  1.2512501948962356\n",
      "step =  24800 , loss_val =  1.174973093119773\n",
      "step =  25200 , loss_val =  1.2021763742925187\n",
      "step =  25600 , loss_val =  1.2242407986900292\n",
      "step =  26000 , loss_val =  1.3458178824938498\n",
      "step =  26400 , loss_val =  1.2711589849340477\n",
      "step =  26800 , loss_val =  1.5127302925959107\n",
      "step =  27200 , loss_val =  1.2051318541480516\n",
      "step =  27600 , loss_val =  1.4727885800581073\n",
      "step =  28000 , loss_val =  1.2123794154946148\n",
      "step =  28400 , loss_val =  1.3282064827905293\n",
      "step =  28800 , loss_val =  1.2939671792648908\n",
      "step =  29200 , loss_val =  1.3768622388324714\n",
      "step =  29600 , loss_val =  1.3170930270338637\n",
      "step =  30000 , loss_val =  1.2998319125404927\n",
      "step =  30400 , loss_val =  1.1039224217676107\n",
      "step =  30800 , loss_val =  1.1212716083799625\n",
      "step =  31200 , loss_val =  1.1278026094973863\n",
      "step =  31600 , loss_val =  1.119474545007186\n",
      "step =  32000 , loss_val =  1.3426462896785845\n",
      "step =  32400 , loss_val =  1.2443074927429014\n",
      "step =  32800 , loss_val =  1.3260171803743412\n",
      "step =  33200 , loss_val =  1.2764896777932424\n",
      "step =  33600 , loss_val =  1.0847251367421924\n",
      "step =  34000 , loss_val =  1.2336542798300818\n",
      "step =  34400 , loss_val =  1.319677929489852\n",
      "step =  34800 , loss_val =  7.127021607791206\n",
      "step =  35200 , loss_val =  1.331321823732068\n",
      "step =  35600 , loss_val =  1.1599174089311026\n",
      "step =  36000 , loss_val =  1.1687310500599153\n",
      "step =  36400 , loss_val =  1.229060996808986\n",
      "step =  36800 , loss_val =  1.2329565498085828\n",
      "step =  37200 , loss_val =  1.25153494824607\n",
      "step =  37600 , loss_val =  1.3602577805143978\n",
      "step =  38000 , loss_val =  1.3081623892176317\n",
      "step =  38400 , loss_val =  1.3185245499668095\n",
      "step =  38800 , loss_val =  1.2904055908335808\n",
      "step =  39200 , loss_val =  1.3206577907414387\n",
      "step =  39600 , loss_val =  1.1295715348900897\n",
      "step =  40000 , loss_val =  1.2677697821075578\n",
      "step =  40400 , loss_val =  1.417459603665518\n",
      "step =  40800 , loss_val =  1.2388036461920982\n",
      "step =  41200 , loss_val =  1.2310016530679844\n",
      "step =  41600 , loss_val =  1.11901649145827\n",
      "step =  42000 , loss_val =  1.274221973364637\n",
      "step =  42400 , loss_val =  1.3061372964440352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  42800 , loss_val =  1.3021082530384782\n",
      "step =  43200 , loss_val =  1.2695476919399742\n",
      "step =  43600 , loss_val =  1.2708124188768974\n",
      "step =  44000 , loss_val =  1.3247597917705036\n",
      "step =  44400 , loss_val =  1.4199258422963161\n",
      "step =  44800 , loss_val =  1.206750008975301\n",
      "step =  45200 , loss_val =  1.4006865254357295\n",
      "step =  45600 , loss_val =  1.2799913155524452\n",
      "step =  46000 , loss_val =  1.2533007240683705\n",
      "step =  46400 , loss_val =  1.197003973186058\n",
      "step =  46800 , loss_val =  1.1815909432950147\n",
      "step =  47200 , loss_val =  1.2020813102792098\n",
      "step =  47600 , loss_val =  6.60367057242831\n",
      "step =  48000 , loss_val =  1.2719836180304585\n",
      "step =  48400 , loss_val =  1.3499424151285897\n",
      "step =  48800 , loss_val =  1.2428807088140035\n",
      "step =  49200 , loss_val =  1.2021074418128344\n",
      "step =  49600 , loss_val =  1.1175354656192373\n",
      "step =  50000 , loss_val =  1.3416068435773187\n",
      "step =  50400 , loss_val =  1.1178293017587482\n",
      "step =  50800 , loss_val =  1.4006102026535714\n",
      "step =  51200 , loss_val =  1.0455579960495596\n",
      "step =  51600 , loss_val =  5.3076345975176595\n",
      "step =  52000 , loss_val =  1.319812733951999\n",
      "step =  52400 , loss_val =  1.3837164679975984\n",
      "step =  52800 , loss_val =  15.876759315873159\n",
      "step =  53200 , loss_val =  1.303477198965742\n",
      "step =  53600 , loss_val =  1.2118030512085598\n",
      "step =  54000 , loss_val =  1.3570044569409554\n",
      "step =  54400 , loss_val =  1.3169718417371408\n",
      "step =  54800 , loss_val =  1.2877882079015692\n",
      "step =  55200 , loss_val =  1.1734091047857083\n",
      "step =  55600 , loss_val =  1.201922616277474\n",
      "step =  56000 , loss_val =  1.1746895052966058\n",
      "step =  56400 , loss_val =  1.2020470526637157\n",
      "step =  56800 , loss_val =  15.546879425217021\n",
      "step =  57200 , loss_val =  1.3092913610948007\n",
      "step =  57600 , loss_val =  1.2677544933539502\n",
      "step =  58000 , loss_val =  1.3932891135320422\n",
      "step =  58400 , loss_val =  1.2497336373376111\n",
      "step =  58800 , loss_val =  1.3227930274221913\n",
      "step =  59200 , loss_val =  1.1690161957165959\n",
      "step =  59600 , loss_val =  1.3424532114019674\n",
      "\n",
      " time =  0:01:17.870980\n"
     ]
    }
   ],
   "source": [
    "input_nodes = 784\n",
    "hidden_nodes = 100\n",
    "output_nodes = 10\n",
    "learning_rate = 0.3\n",
    "epochs = 5\n",
    "\n",
    "nn = NeuralNetwork(input_nodes,hidden_nodes,output_nodes,learning_rate)\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for i in range(epochs):\n",
    "    for step in range(len(training_data)):\n",
    "        \n",
    "        #input_data, target_data normalize\n",
    "        #0과1사이의 값을 갖도록 정규화\n",
    "        target_data = np.zeros(output_nodes) + 0.01\n",
    "        target_data[int(training_data[step,0])] = 0.99\n",
    "        \n",
    "        input_data = ((training_data[step,1:]/255.0)*0.99)+0.01\n",
    "        \n",
    "        nn.train(np.array(input_data,ndmin=2),np.array(target_data,ndmin=2))\n",
    "        \n",
    "        if step%400 == 0:\n",
    "            print(\"step = \",step, \", loss_val = \",nn.loss_val())\n",
    "end_time = datetime.now()\n",
    "print(\"\\n time = \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Accuracy =  96.71  %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40,\n",
       "  41,\n",
       "  42,\n",
       "  43,\n",
       "  44,\n",
       "  45,\n",
       "  46,\n",
       "  47,\n",
       "  48,\n",
       "  49,\n",
       "  50,\n",
       "  51,\n",
       "  52,\n",
       "  53,\n",
       "  54,\n",
       "  55,\n",
       "  56,\n",
       "  57,\n",
       "  58,\n",
       "  59,\n",
       "  60,\n",
       "  61,\n",
       "  62,\n",
       "  63,\n",
       "  64,\n",
       "  65,\n",
       "  66,\n",
       "  67,\n",
       "  68,\n",
       "  69,\n",
       "  70,\n",
       "  71,\n",
       "  72,\n",
       "  73,\n",
       "  74,\n",
       "  75,\n",
       "  76,\n",
       "  77,\n",
       "  78,\n",
       "  79,\n",
       "  80,\n",
       "  81,\n",
       "  82,\n",
       "  83,\n",
       "  84,\n",
       "  85,\n",
       "  86,\n",
       "  87,\n",
       "  88,\n",
       "  89,\n",
       "  90,\n",
       "  91,\n",
       "  92,\n",
       "  93,\n",
       "  94,\n",
       "  95,\n",
       "  96,\n",
       "  97,\n",
       "  98,\n",
       "  99,\n",
       "  100,\n",
       "  101,\n",
       "  102,\n",
       "  103,\n",
       "  104,\n",
       "  105,\n",
       "  106,\n",
       "  107,\n",
       "  108,\n",
       "  109,\n",
       "  110,\n",
       "  111,\n",
       "  112,\n",
       "  113,\n",
       "  114,\n",
       "  115,\n",
       "  116,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  121,\n",
       "  122,\n",
       "  123,\n",
       "  124,\n",
       "  125,\n",
       "  126,\n",
       "  127,\n",
       "  128,\n",
       "  129,\n",
       "  130,\n",
       "  131,\n",
       "  132,\n",
       "  133,\n",
       "  134,\n",
       "  135,\n",
       "  136,\n",
       "  137,\n",
       "  138,\n",
       "  139,\n",
       "  140,\n",
       "  141,\n",
       "  142,\n",
       "  143,\n",
       "  144,\n",
       "  145,\n",
       "  146,\n",
       "  147,\n",
       "  148,\n",
       "  150,\n",
       "  151,\n",
       "  152,\n",
       "  153,\n",
       "  154,\n",
       "  155,\n",
       "  156,\n",
       "  157,\n",
       "  158,\n",
       "  159,\n",
       "  160,\n",
       "  161,\n",
       "  162,\n",
       "  163,\n",
       "  164,\n",
       "  165,\n",
       "  166,\n",
       "  167,\n",
       "  168,\n",
       "  169,\n",
       "  170,\n",
       "  171,\n",
       "  172,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  179,\n",
       "  180,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186,\n",
       "  187,\n",
       "  188,\n",
       "  189,\n",
       "  190,\n",
       "  191,\n",
       "  192,\n",
       "  193,\n",
       "  194,\n",
       "  195,\n",
       "  196,\n",
       "  197,\n",
       "  198,\n",
       "  199,\n",
       "  200,\n",
       "  201,\n",
       "  202,\n",
       "  203,\n",
       "  204,\n",
       "  205,\n",
       "  206,\n",
       "  207,\n",
       "  208,\n",
       "  209,\n",
       "  210,\n",
       "  211,\n",
       "  212,\n",
       "  213,\n",
       "  214,\n",
       "  215,\n",
       "  216,\n",
       "  217,\n",
       "  218,\n",
       "  219,\n",
       "  220,\n",
       "  221,\n",
       "  222,\n",
       "  223,\n",
       "  224,\n",
       "  225,\n",
       "  226,\n",
       "  227,\n",
       "  228,\n",
       "  229,\n",
       "  230,\n",
       "  231,\n",
       "  232,\n",
       "  233,\n",
       "  234,\n",
       "  235,\n",
       "  236,\n",
       "  237,\n",
       "  238,\n",
       "  239,\n",
       "  240,\n",
       "  241,\n",
       "  242,\n",
       "  243,\n",
       "  244,\n",
       "  245,\n",
       "  246,\n",
       "  248,\n",
       "  249,\n",
       "  250,\n",
       "  251,\n",
       "  252,\n",
       "  253,\n",
       "  254,\n",
       "  255,\n",
       "  256,\n",
       "  257,\n",
       "  258,\n",
       "  260,\n",
       "  261,\n",
       "  262,\n",
       "  263,\n",
       "  264,\n",
       "  265,\n",
       "  266,\n",
       "  267,\n",
       "  268,\n",
       "  269,\n",
       "  270,\n",
       "  271,\n",
       "  272,\n",
       "  273,\n",
       "  274,\n",
       "  275,\n",
       "  276,\n",
       "  277,\n",
       "  278,\n",
       "  279,\n",
       "  280,\n",
       "  281,\n",
       "  282,\n",
       "  283,\n",
       "  284,\n",
       "  285,\n",
       "  286,\n",
       "  287,\n",
       "  288,\n",
       "  289,\n",
       "  290,\n",
       "  291,\n",
       "  292,\n",
       "  293,\n",
       "  294,\n",
       "  295,\n",
       "  296,\n",
       "  297,\n",
       "  298,\n",
       "  299,\n",
       "  300,\n",
       "  301,\n",
       "  302,\n",
       "  303,\n",
       "  304,\n",
       "  305,\n",
       "  306,\n",
       "  307,\n",
       "  308,\n",
       "  309,\n",
       "  310,\n",
       "  311,\n",
       "  312,\n",
       "  313,\n",
       "  314,\n",
       "  315,\n",
       "  316,\n",
       "  317,\n",
       "  319,\n",
       "  322,\n",
       "  323,\n",
       "  324,\n",
       "  325,\n",
       "  326,\n",
       "  327,\n",
       "  328,\n",
       "  329,\n",
       "  330,\n",
       "  331,\n",
       "  332,\n",
       "  333,\n",
       "  334,\n",
       "  335,\n",
       "  336,\n",
       "  337,\n",
       "  338,\n",
       "  339,\n",
       "  341,\n",
       "  342,\n",
       "  343,\n",
       "  344,\n",
       "  345,\n",
       "  346,\n",
       "  347,\n",
       "  348,\n",
       "  349,\n",
       "  350,\n",
       "  351,\n",
       "  352,\n",
       "  353,\n",
       "  354,\n",
       "  355,\n",
       "  356,\n",
       "  357,\n",
       "  358,\n",
       "  359,\n",
       "  360,\n",
       "  361,\n",
       "  362,\n",
       "  363,\n",
       "  364,\n",
       "  365,\n",
       "  366,\n",
       "  367,\n",
       "  368,\n",
       "  369,\n",
       "  370,\n",
       "  371,\n",
       "  372,\n",
       "  373,\n",
       "  374,\n",
       "  375,\n",
       "  376,\n",
       "  377,\n",
       "  378,\n",
       "  379,\n",
       "  380,\n",
       "  381,\n",
       "  382,\n",
       "  383,\n",
       "  384,\n",
       "  385,\n",
       "  386,\n",
       "  387,\n",
       "  388,\n",
       "  389,\n",
       "  390,\n",
       "  391,\n",
       "  392,\n",
       "  393,\n",
       "  394,\n",
       "  395,\n",
       "  396,\n",
       "  397,\n",
       "  398,\n",
       "  399,\n",
       "  400,\n",
       "  401,\n",
       "  402,\n",
       "  403,\n",
       "  404,\n",
       "  405,\n",
       "  406,\n",
       "  407,\n",
       "  408,\n",
       "  409,\n",
       "  410,\n",
       "  411,\n",
       "  412,\n",
       "  413,\n",
       "  414,\n",
       "  415,\n",
       "  416,\n",
       "  417,\n",
       "  418,\n",
       "  419,\n",
       "  420,\n",
       "  421,\n",
       "  422,\n",
       "  423,\n",
       "  424,\n",
       "  425,\n",
       "  426,\n",
       "  427,\n",
       "  428,\n",
       "  429,\n",
       "  430,\n",
       "  431,\n",
       "  432,\n",
       "  433,\n",
       "  434,\n",
       "  435,\n",
       "  436,\n",
       "  437,\n",
       "  438,\n",
       "  439,\n",
       "  440,\n",
       "  441,\n",
       "  442,\n",
       "  443,\n",
       "  444,\n",
       "  445,\n",
       "  446,\n",
       "  450,\n",
       "  451,\n",
       "  452,\n",
       "  453,\n",
       "  454,\n",
       "  455,\n",
       "  456,\n",
       "  457,\n",
       "  458,\n",
       "  459,\n",
       "  460,\n",
       "  461,\n",
       "  462,\n",
       "  463,\n",
       "  464,\n",
       "  465,\n",
       "  466,\n",
       "  467,\n",
       "  468,\n",
       "  469,\n",
       "  470,\n",
       "  471,\n",
       "  472,\n",
       "  473,\n",
       "  474,\n",
       "  475,\n",
       "  476,\n",
       "  477,\n",
       "  478,\n",
       "  480,\n",
       "  481,\n",
       "  482,\n",
       "  483,\n",
       "  484,\n",
       "  485,\n",
       "  486,\n",
       "  487,\n",
       "  488,\n",
       "  489,\n",
       "  490,\n",
       "  491,\n",
       "  492,\n",
       "  493,\n",
       "  494,\n",
       "  496,\n",
       "  497,\n",
       "  498,\n",
       "  499,\n",
       "  500,\n",
       "  501,\n",
       "  502,\n",
       "  503,\n",
       "  504,\n",
       "  505,\n",
       "  506,\n",
       "  507,\n",
       "  508,\n",
       "  509,\n",
       "  510,\n",
       "  511,\n",
       "  512,\n",
       "  513,\n",
       "  514,\n",
       "  515,\n",
       "  516,\n",
       "  517,\n",
       "  518,\n",
       "  519,\n",
       "  520,\n",
       "  521,\n",
       "  522,\n",
       "  523,\n",
       "  524,\n",
       "  525,\n",
       "  526,\n",
       "  527,\n",
       "  528,\n",
       "  529,\n",
       "  530,\n",
       "  531,\n",
       "  532,\n",
       "  533,\n",
       "  534,\n",
       "  535,\n",
       "  536,\n",
       "  537,\n",
       "  538,\n",
       "  539,\n",
       "  540,\n",
       "  541,\n",
       "  542,\n",
       "  543,\n",
       "  544,\n",
       "  545,\n",
       "  546,\n",
       "  547,\n",
       "  548,\n",
       "  549,\n",
       "  550,\n",
       "  551,\n",
       "  552,\n",
       "  553,\n",
       "  554,\n",
       "  555,\n",
       "  556,\n",
       "  557,\n",
       "  558,\n",
       "  559,\n",
       "  560,\n",
       "  561,\n",
       "  562,\n",
       "  563,\n",
       "  564,\n",
       "  565,\n",
       "  566,\n",
       "  567,\n",
       "  568,\n",
       "  569,\n",
       "  570,\n",
       "  572,\n",
       "  573,\n",
       "  574,\n",
       "  575,\n",
       "  576,\n",
       "  577,\n",
       "  579,\n",
       "  580,\n",
       "  581,\n",
       "  583,\n",
       "  584,\n",
       "  585,\n",
       "  586,\n",
       "  587,\n",
       "  588,\n",
       "  589,\n",
       "  590,\n",
       "  591,\n",
       "  592,\n",
       "  593,\n",
       "  594,\n",
       "  595,\n",
       "  596,\n",
       "  597,\n",
       "  598,\n",
       "  599,\n",
       "  600,\n",
       "  601,\n",
       "  602,\n",
       "  603,\n",
       "  604,\n",
       "  605,\n",
       "  606,\n",
       "  607,\n",
       "  608,\n",
       "  609,\n",
       "  610,\n",
       "  611,\n",
       "  612,\n",
       "  613,\n",
       "  614,\n",
       "  615,\n",
       "  616,\n",
       "  617,\n",
       "  618,\n",
       "  619,\n",
       "  620,\n",
       "  621,\n",
       "  622,\n",
       "  623,\n",
       "  624,\n",
       "  625,\n",
       "  626,\n",
       "  627,\n",
       "  628,\n",
       "  629,\n",
       "  630,\n",
       "  631,\n",
       "  632,\n",
       "  633,\n",
       "  634,\n",
       "  635,\n",
       "  636,\n",
       "  637,\n",
       "  638,\n",
       "  639,\n",
       "  640,\n",
       "  641,\n",
       "  642,\n",
       "  643,\n",
       "  644,\n",
       "  645,\n",
       "  647,\n",
       "  648,\n",
       "  649,\n",
       "  650,\n",
       "  651,\n",
       "  652,\n",
       "  653,\n",
       "  654,\n",
       "  655,\n",
       "  656,\n",
       "  657,\n",
       "  660,\n",
       "  661,\n",
       "  662,\n",
       "  663,\n",
       "  664,\n",
       "  665,\n",
       "  666,\n",
       "  667,\n",
       "  668,\n",
       "  669,\n",
       "  670,\n",
       "  671,\n",
       "  672,\n",
       "  673,\n",
       "  674,\n",
       "  675,\n",
       "  676,\n",
       "  677,\n",
       "  678,\n",
       "  679,\n",
       "  680,\n",
       "  681,\n",
       "  682,\n",
       "  683,\n",
       "  684,\n",
       "  685,\n",
       "  686,\n",
       "  687,\n",
       "  688,\n",
       "  689,\n",
       "  690,\n",
       "  692,\n",
       "  693,\n",
       "  694,\n",
       "  695,\n",
       "  696,\n",
       "  697,\n",
       "  698,\n",
       "  699,\n",
       "  700,\n",
       "  701,\n",
       "  702,\n",
       "  703,\n",
       "  704,\n",
       "  705,\n",
       "  706,\n",
       "  708,\n",
       "  709,\n",
       "  710,\n",
       "  711,\n",
       "  712,\n",
       "  713,\n",
       "  714,\n",
       "  715,\n",
       "  716,\n",
       "  718,\n",
       "  719,\n",
       "  721,\n",
       "  722,\n",
       "  723,\n",
       "  724,\n",
       "  725,\n",
       "  727,\n",
       "  728,\n",
       "  729,\n",
       "  730,\n",
       "  731,\n",
       "  732,\n",
       "  733,\n",
       "  734,\n",
       "  735,\n",
       "  736,\n",
       "  737,\n",
       "  738,\n",
       "  739,\n",
       "  741,\n",
       "  742,\n",
       "  743,\n",
       "  744,\n",
       "  745,\n",
       "  746,\n",
       "  747,\n",
       "  748,\n",
       "  749,\n",
       "  750,\n",
       "  751,\n",
       "  752,\n",
       "  753,\n",
       "  754,\n",
       "  755,\n",
       "  756,\n",
       "  757,\n",
       "  758,\n",
       "  759,\n",
       "  761,\n",
       "  762,\n",
       "  763,\n",
       "  764,\n",
       "  765,\n",
       "  766,\n",
       "  767,\n",
       "  768,\n",
       "  769,\n",
       "  770,\n",
       "  771,\n",
       "  772,\n",
       "  773,\n",
       "  774,\n",
       "  775,\n",
       "  776,\n",
       "  777,\n",
       "  778,\n",
       "  779,\n",
       "  780,\n",
       "  781,\n",
       "  782,\n",
       "  783,\n",
       "  784,\n",
       "  785,\n",
       "  786,\n",
       "  787,\n",
       "  788,\n",
       "  789,\n",
       "  790,\n",
       "  791,\n",
       "  792,\n",
       "  793,\n",
       "  794,\n",
       "  795,\n",
       "  796,\n",
       "  797,\n",
       "  798,\n",
       "  799,\n",
       "  800,\n",
       "  801,\n",
       "  802,\n",
       "  803,\n",
       "  804,\n",
       "  805,\n",
       "  806,\n",
       "  807,\n",
       "  808,\n",
       "  809,\n",
       "  811,\n",
       "  812,\n",
       "  813,\n",
       "  814,\n",
       "  815,\n",
       "  816,\n",
       "  817,\n",
       "  818,\n",
       "  819,\n",
       "  820,\n",
       "  821,\n",
       "  822,\n",
       "  823,\n",
       "  824,\n",
       "  825,\n",
       "  826,\n",
       "  827,\n",
       "  828,\n",
       "  829,\n",
       "  830,\n",
       "  831,\n",
       "  832,\n",
       "  833,\n",
       "  834,\n",
       "  835,\n",
       "  836,\n",
       "  837,\n",
       "  838,\n",
       "  839,\n",
       "  840,\n",
       "  841,\n",
       "  842,\n",
       "  843,\n",
       "  845,\n",
       "  846,\n",
       "  847,\n",
       "  848,\n",
       "  849,\n",
       "  850,\n",
       "  851,\n",
       "  852,\n",
       "  853,\n",
       "  854,\n",
       "  855,\n",
       "  856,\n",
       "  857,\n",
       "  858,\n",
       "  859,\n",
       "  860,\n",
       "  861,\n",
       "  862,\n",
       "  863,\n",
       "  864,\n",
       "  865,\n",
       "  866,\n",
       "  867,\n",
       "  868,\n",
       "  869,\n",
       "  870,\n",
       "  871,\n",
       "  872,\n",
       "  873,\n",
       "  874,\n",
       "  875,\n",
       "  876,\n",
       "  877,\n",
       "  878,\n",
       "  879,\n",
       "  880,\n",
       "  881,\n",
       "  883,\n",
       "  884,\n",
       "  885,\n",
       "  886,\n",
       "  887,\n",
       "  888,\n",
       "  889,\n",
       "  890,\n",
       "  891,\n",
       "  892,\n",
       "  893,\n",
       "  894,\n",
       "  895,\n",
       "  896,\n",
       "  897,\n",
       "  898,\n",
       "  899,\n",
       "  900,\n",
       "  901,\n",
       "  902,\n",
       "  903,\n",
       "  904,\n",
       "  905,\n",
       "  906,\n",
       "  907,\n",
       "  908,\n",
       "  909,\n",
       "  910,\n",
       "  911,\n",
       "  912,\n",
       "  913,\n",
       "  914,\n",
       "  915,\n",
       "  916,\n",
       "  917,\n",
       "  918,\n",
       "  919,\n",
       "  920,\n",
       "  921,\n",
       "  922,\n",
       "  923,\n",
       "  924,\n",
       "  925,\n",
       "  926,\n",
       "  927,\n",
       "  928,\n",
       "  929,\n",
       "  930,\n",
       "  931,\n",
       "  932,\n",
       "  933,\n",
       "  934,\n",
       "  935,\n",
       "  936,\n",
       "  937,\n",
       "  938,\n",
       "  939,\n",
       "  940,\n",
       "  941,\n",
       "  942,\n",
       "  943,\n",
       "  944,\n",
       "  945,\n",
       "  946,\n",
       "  948,\n",
       "  949,\n",
       "  950,\n",
       "  951,\n",
       "  952,\n",
       "  953,\n",
       "  954,\n",
       "  955,\n",
       "  956,\n",
       "  957,\n",
       "  958,\n",
       "  959,\n",
       "  960,\n",
       "  961,\n",
       "  962,\n",
       "  963,\n",
       "  964,\n",
       "  966,\n",
       "  967,\n",
       "  968,\n",
       "  969,\n",
       "  970,\n",
       "  971,\n",
       "  972,\n",
       "  973,\n",
       "  974,\n",
       "  975,\n",
       "  976,\n",
       "  977,\n",
       "  978,\n",
       "  979,\n",
       "  980,\n",
       "  981,\n",
       "  982,\n",
       "  983,\n",
       "  984,\n",
       "  985,\n",
       "  986,\n",
       "  987,\n",
       "  988,\n",
       "  989,\n",
       "  990,\n",
       "  991,\n",
       "  992,\n",
       "  993,\n",
       "  994,\n",
       "  995,\n",
       "  996,\n",
       "  997,\n",
       "  998,\n",
       "  999,\n",
       "  1000,\n",
       "  1001,\n",
       "  1002,\n",
       "  1003,\n",
       "  1004,\n",
       "  1005,\n",
       "  1006,\n",
       "  1007,\n",
       "  1008,\n",
       "  1009,\n",
       "  1010,\n",
       "  1011,\n",
       "  1012,\n",
       "  1013,\n",
       "  1015,\n",
       "  1016,\n",
       "  1017,\n",
       "  1018,\n",
       "  1019,\n",
       "  1020,\n",
       "  1021,\n",
       "  1022,\n",
       "  1023,\n",
       "  1024,\n",
       "  1025,\n",
       "  1026,\n",
       "  1027,\n",
       "  1028,\n",
       "  1029,\n",
       "  1030,\n",
       "  ...],\n",
       " [149,\n",
       "  247,\n",
       "  259,\n",
       "  318,\n",
       "  320,\n",
       "  321,\n",
       "  340,\n",
       "  447,\n",
       "  448,\n",
       "  449,\n",
       "  479,\n",
       "  495,\n",
       "  571,\n",
       "  578,\n",
       "  582,\n",
       "  646,\n",
       "  658,\n",
       "  659,\n",
       "  691,\n",
       "  707,\n",
       "  717,\n",
       "  720,\n",
       "  726,\n",
       "  740,\n",
       "  760,\n",
       "  810,\n",
       "  844,\n",
       "  882,\n",
       "  947,\n",
       "  965,\n",
       "  1014,\n",
       "  1039,\n",
       "  1044,\n",
       "  1062,\n",
       "  1107,\n",
       "  1112,\n",
       "  1114,\n",
       "  1128,\n",
       "  1181,\n",
       "  1192,\n",
       "  1194,\n",
       "  1204,\n",
       "  1226,\n",
       "  1232,\n",
       "  1242,\n",
       "  1247,\n",
       "  1260,\n",
       "  1283,\n",
       "  1289,\n",
       "  1299,\n",
       "  1319,\n",
       "  1325,\n",
       "  1326,\n",
       "  1328,\n",
       "  1378,\n",
       "  1422,\n",
       "  1433,\n",
       "  1444,\n",
       "  1467,\n",
       "  1494,\n",
       "  1500,\n",
       "  1522,\n",
       "  1525,\n",
       "  1530,\n",
       "  1549,\n",
       "  1553,\n",
       "  1609,\n",
       "  1621,\n",
       "  1626,\n",
       "  1681,\n",
       "  1709,\n",
       "  1717,\n",
       "  1754,\n",
       "  1790,\n",
       "  1878,\n",
       "  1901,\n",
       "  1938,\n",
       "  1940,\n",
       "  1941,\n",
       "  1952,\n",
       "  1982,\n",
       "  2016,\n",
       "  2024,\n",
       "  2035,\n",
       "  2043,\n",
       "  2044,\n",
       "  2053,\n",
       "  2070,\n",
       "  2093,\n",
       "  2098,\n",
       "  2109,\n",
       "  2118,\n",
       "  2125,\n",
       "  2129,\n",
       "  2130,\n",
       "  2135,\n",
       "  2148,\n",
       "  2182,\n",
       "  2185,\n",
       "  2186,\n",
       "  2215,\n",
       "  2224,\n",
       "  2266,\n",
       "  2272,\n",
       "  2293,\n",
       "  2299,\n",
       "  2326,\n",
       "  2369,\n",
       "  2371,\n",
       "  2380,\n",
       "  2406,\n",
       "  2414,\n",
       "  2454,\n",
       "  2462,\n",
       "  2488,\n",
       "  2514,\n",
       "  2578,\n",
       "  2598,\n",
       "  2607,\n",
       "  2648,\n",
       "  2654,\n",
       "  2713,\n",
       "  2720,\n",
       "  2780,\n",
       "  2877,\n",
       "  2896,\n",
       "  2907,\n",
       "  2921,\n",
       "  2925,\n",
       "  2927,\n",
       "  2939,\n",
       "  2945,\n",
       "  2953,\n",
       "  2995,\n",
       "  3005,\n",
       "  3060,\n",
       "  3062,\n",
       "  3073,\n",
       "  3114,\n",
       "  3117,\n",
       "  3167,\n",
       "  3189,\n",
       "  3218,\n",
       "  3240,\n",
       "  3284,\n",
       "  3333,\n",
       "  3376,\n",
       "  3384,\n",
       "  3405,\n",
       "  3503,\n",
       "  3520,\n",
       "  3533,\n",
       "  3549,\n",
       "  3558,\n",
       "  3559,\n",
       "  3567,\n",
       "  3597,\n",
       "  3604,\n",
       "  3618,\n",
       "  3716,\n",
       "  3718,\n",
       "  3726,\n",
       "  3730,\n",
       "  3749,\n",
       "  3757,\n",
       "  3767,\n",
       "  3780,\n",
       "  3806,\n",
       "  3808,\n",
       "  3811,\n",
       "  3838,\n",
       "  3853,\n",
       "  3869,\n",
       "  3893,\n",
       "  3902,\n",
       "  3906,\n",
       "  3926,\n",
       "  3941,\n",
       "  3962,\n",
       "  3968,\n",
       "  3985,\n",
       "  4017,\n",
       "  4027,\n",
       "  4063,\n",
       "  4065,\n",
       "  4075,\n",
       "  4078,\n",
       "  4140,\n",
       "  4152,\n",
       "  4163,\n",
       "  4176,\n",
       "  4199,\n",
       "  4205,\n",
       "  4211,\n",
       "  4212,\n",
       "  4224,\n",
       "  4248,\n",
       "  4265,\n",
       "  4289,\n",
       "  4306,\n",
       "  4355,\n",
       "  4382,\n",
       "  4435,\n",
       "  4497,\n",
       "  4536,\n",
       "  4571,\n",
       "  4575,\n",
       "  4578,\n",
       "  4601,\n",
       "  4635,\n",
       "  4690,\n",
       "  4731,\n",
       "  4751,\n",
       "  4761,\n",
       "  4807,\n",
       "  4814,\n",
       "  4823,\n",
       "  4829,\n",
       "  4837,\n",
       "  4860,\n",
       "  4874,\n",
       "  4876,\n",
       "  4880,\n",
       "  4886,\n",
       "  4950,\n",
       "  4956,\n",
       "  4966,\n",
       "  4990,\n",
       "  5067,\n",
       "  5140,\n",
       "  5159,\n",
       "  5165,\n",
       "  5246,\n",
       "  5331,\n",
       "  5457,\n",
       "  5600,\n",
       "  5623,\n",
       "  5642,\n",
       "  5734,\n",
       "  5749,\n",
       "  5835,\n",
       "  5842,\n",
       "  5887,\n",
       "  5888,\n",
       "  5891,\n",
       "  5906,\n",
       "  5922,\n",
       "  5936,\n",
       "  5937,\n",
       "  5955,\n",
       "  5972,\n",
       "  5973,\n",
       "  5982,\n",
       "  6035,\n",
       "  6042,\n",
       "  6045,\n",
       "  6059,\n",
       "  6071,\n",
       "  6081,\n",
       "  6091,\n",
       "  6166,\n",
       "  6172,\n",
       "  6173,\n",
       "  6390,\n",
       "  6391,\n",
       "  6400,\n",
       "  6421,\n",
       "  6425,\n",
       "  6505,\n",
       "  6558,\n",
       "  6560,\n",
       "  6577,\n",
       "  6597,\n",
       "  6598,\n",
       "  6625,\n",
       "  6632,\n",
       "  6645,\n",
       "  6651,\n",
       "  6769,\n",
       "  7049,\n",
       "  7216,\n",
       "  7248,\n",
       "  7432,\n",
       "  7434,\n",
       "  7451,\n",
       "  7637,\n",
       "  7797,\n",
       "  7821,\n",
       "  7849,\n",
       "  7886,\n",
       "  7917,\n",
       "  8062,\n",
       "  8094,\n",
       "  8272,\n",
       "  8339,\n",
       "  8362,\n",
       "  8406,\n",
       "  8408,\n",
       "  8522,\n",
       "  8527,\n",
       "  9009,\n",
       "  9015,\n",
       "  9016,\n",
       "  9024,\n",
       "  9280,\n",
       "  9422,\n",
       "  9587,\n",
       "  9634,\n",
       "  9664,\n",
       "  9669,\n",
       "  9679,\n",
       "  9692,\n",
       "  9698,\n",
       "  9700,\n",
       "  9716,\n",
       "  9729,\n",
       "  9744,\n",
       "  9745,\n",
       "  9749,\n",
       "  9768,\n",
       "  9770,\n",
       "  9777,\n",
       "  9779,\n",
       "  9839,\n",
       "  9888,\n",
       "  9905,\n",
       "  9925,\n",
       "  9944,\n",
       "  9982])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.accuracy(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
